{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a25e3a2-f015-4ed2-8e62-b7eedcc4d2ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from Architecture import anti_spoofing\n",
    "import time\n",
    "import dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcdedca-eaca-44c5-b9fc-83e206dbb427",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "image1 = face_recognition.load_image_file(\"1.jpg\")\n",
    "image_encode1 = face_recognition.face_encodings(image1)[0]\n",
    "\n",
    "image2 = face_recognition.load_image_file(\"3.jpg\")\n",
    "image_encode2 = face_recognition.face_encodings(image2)[0]\n",
    "\n",
    "known_faces = [image_encode1, image_encode2]\n",
    "known_names = ['bassem', 'Soudy']\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = anti_spoofing().to(device)\n",
    "weights_path = 'model.pth'\n",
    "state_dict = torch.load(weights_path, map_location=torch.device('cpu'))\n",
    "new_state_dict = OrderedDict()\n",
    "for key, value in state_dict.items():\n",
    "    name_key = key[7:]\n",
    "    new_state_dict[name_key] = value\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.eval()\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "    faces = detector(frame)\n",
    "\n",
    "    for face in faces:\n",
    "        x, y, width, height = face.left(), face.top(), face.width(), face.height()\n",
    "        h, w, c = np.shape(frame)\n",
    "\n",
    "        scale = min((h - 1) / height, min((w - 1) / width, 2.7))\n",
    "        new_width = width * scale\n",
    "        new_height = height * scale\n",
    "        center_x, center_y = width / 2 + x, height / 2 + y\n",
    "\n",
    "        left_top_x = center_x - new_width / 2\n",
    "        left_top_y = center_y - new_height / 2\n",
    "        right_bottom_x = center_x + new_width / 2\n",
    "        right_bottom_y = center_y + new_height / 2\n",
    "\n",
    "        left_top_x = max(0, int(left_top_x))\n",
    "        left_top_y = max(0, int(left_top_y))\n",
    "        right_bottom_x = min(w - 1, int(right_bottom_x))\n",
    "        right_bottom_y = min(h - 1, int(right_bottom_y))\n",
    "\n",
    "        face_region = frame[left_top_y:right_bottom_y + 1, left_top_x:right_bottom_x + 1]\n",
    "        face_region = cv2.resize(face_region, (80, 80))\n",
    "        face_tensor = torch.from_numpy(face_region.transpose((2, 0, 1))).float()\n",
    "        face_tensor = face_tensor.unsqueeze(0).to(torch.device('cpu'))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model.forward(face_tensor)\n",
    "            result = F.softmax(output, dim=-1).cpu().numpy()\n",
    "\n",
    "        label = np.argmax(result)\n",
    "        value = result[0][label]\n",
    "\n",
    "        if label == 1:\n",
    "            new_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            locations = face_recognition.face_locations(new_frame)\n",
    "            faces = face_recognition.face_encodings(new_frame, locations)\n",
    "            \n",
    "            names = []\n",
    "            for face in faces:\n",
    "                compare = face_recognition.compare_faces(known_faces, face)\n",
    "                similarity = face_recognition.face_distance(known_faces, face)\n",
    "                if compare[np.argmin(similarity)]:\n",
    "                    name = known_names[np.argmin(similarity)]\n",
    "                else:\n",
    "                    name = \"Unknown\"\n",
    "                names.append(name)\n",
    "\n",
    "            for (y_, h_, w_, x_), name in zip(locations, names):\n",
    "                cv2.rectangle(frame, (x_, y_), (h_, w_), (255, 0, 0), 2)\n",
    "                cv2.putText(frame, name, (x_ + 6, y_ - 40), cv2.FONT_HERSHEY_DUPLEX, 1.0, (0, 0, 255), 1)\n",
    "\n",
    "        else:\n",
    "            # print(\"Fake Face Detected. Score: {:.2f}\".format(value))\n",
    "            cv2.rectangle(frame, (x, y), (x + width, y + height), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, 'Fake', (x + 6, y - 40), cv2.FONT_HERSHEY_DUPLEX, 1.0, (0, 0, 255), 1)\n",
    "\n",
    "        cv2.imshow('Video', frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa893be6-ba6a-4240-ade1-a3c31abbcb76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from Architecture import anti_spoofing\n",
    "import time\n",
    "\n",
    "image1 = face_recognition.load_image_file(\"1.jpg\")\n",
    "image_encode1 = face_recognition.face_encodings(image1)[0]\n",
    "\n",
    "image2 = face_recognition.load_image_file(\"3.jpg\")\n",
    "image_encode2 = face_recognition.face_encodings(image2)[0]\n",
    "\n",
    "known_faces = [image_encode1, image_encode2]\n",
    "known_names = ['bassem', 'Soudy']\n",
    "\n",
    "\n",
    "ima = cv2.imread('1.jpg')\n",
    "\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "model = anti_spoofing().to(device)\n",
    "weights_path = 'model.pth'\n",
    "state_dict = torch.load(weights_path, map_location=torch.device('cpu'))\n",
    "new_state_dict = OrderedDict()\n",
    "for key, value in state_dict.items():\n",
    "    name_key = key[7:]\n",
    "    new_state_dict[name_key] = value\n",
    "model.load_state_dict(new_state_dict)\n",
    "model.eval()\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "faces = detector(ima)\n",
    "\n",
    "for face in faces:\n",
    "    x, y, width, height = face.left(), face.top(), face.width(), face.height()\n",
    "    h, w, c = np.shape(frame)\n",
    "\n",
    "    scale = min((h - 1) / height, min((w - 1) / width, 2.7))\n",
    "    new_width = width * scale\n",
    "    new_height = height * scale\n",
    "    center_x, center_y = width / 2 + x, height / 2 + y\n",
    "\n",
    "    left_top_x = center_x - new_width / 2\n",
    "    left_top_y = center_y - new_height / 2\n",
    "    right_bottom_x = center_x + new_width / 2\n",
    "    right_bottom_y = center_y + new_height / 2\n",
    "\n",
    "    left_top_x = max(0, int(left_top_x))\n",
    "    left_top_y = max(0, int(left_top_y))\n",
    "    right_bottom_x = min(w - 1, int(right_bottom_x))\n",
    "    right_bottom_y = min(h - 1, int(right_bottom_y))\n",
    "\n",
    "    face_region = frame[left_top_y:right_bottom_y + 1, left_top_x:right_bottom_x + 1]\n",
    "    face_region = cv2.resize(face_region, (80, 80))\n",
    "    face_tensor = torch.from_numpy(face_region.transpose((2, 0, 1))).float()\n",
    "    face_tensor = face_tensor.unsqueeze(0).to(torch.device('cpu'))\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model.forward(face_tensor)\n",
    "        result = F.softmax(output, dim=-1).cpu().numpy()\n",
    "\n",
    "    label = np.argmax(result)\n",
    "    value = result[0][label]\n",
    "\n",
    "    if label == 1:\n",
    "        new_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        locations = face_recognition.face_locations(new_frame)\n",
    "        faces = face_recognition.face_encodings(new_frame, locations)\n",
    "\n",
    "        names = []\n",
    "        for face in faces:\n",
    "            compare = face_recognition.compare_faces(known_faces, face)\n",
    "            similarity = face_recognition.face_distance(known_faces, face)\n",
    "            if compare[np.argmin(similarity)]:\n",
    "                name = known_names[np.argmin(similarity)]\n",
    "            else:\n",
    "                name = \"Unknown\"\n",
    "            names.append(name)\n",
    "\n",
    "        for (y_, h_, w_, x_), name in zip(locations, names):\n",
    "            cv2.rectangle(frame, (x_, y_), (h_, w_), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, name, (x_ + 6, y_ - 40), cv2.FONT_HERSHEY_DUPLEX, 1.0, (0, 0, 255), 1)\n",
    "\n",
    "    else:\n",
    "        # print(\"Fake Face Detected. Score: {:.2f}\".format(value))\n",
    "            cv2.rectangle(frame, (x, y), (x + width, y + height), (255, 0, 0), 2)\n",
    "            cv2.putText(frame, 'Fake', (x + 6, y - 40), cv2.FONT_HERSHEY_DUPLEX, 1.0, (0, 0, 255), 1)\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0220ea5c-c8f3-4eeb-9b4b-60ef71b77ceb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(ima)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf1b3cb-bce5-4a0a-9d80-e90f965c32dc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "face",
   "language": "python",
   "name": "face"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
